num training examples:  3276
num testing examples:  820
GNN2(
  (GCNconv1): GCNConv(8, 128)
  (GCNconv2): GCNConv(128, 64)
  (GCNconv3): GCNConv(64, 32)
  (GCNconv4): GCNConv(32, 16)
  (CNNconv1): Conv1d(16, 64, kernel_size=(10,), stride=(1,))
  (CNNconv2): Conv1d(64, 32, kernel_size=(10,), stride=(1,))
  (CNNconv3): Conv1d(32, 16, kernel_size=(10,), stride=(1,))
  (avg_pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))
  (fc1): Linear(in_features=384, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=1, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
Parameters:  3210081
Epoch 1/100, Loss: 54.8228
Epoch 2/100, Loss: 1.3645
Epoch 3/100, Loss: 0.6427
Epoch 4/100, Loss: 0.5780
Epoch 5/100, Loss: 0.5619
Epoch 6/100, Loss: 0.5645
Epoch 7/100, Loss: 0.5711
Epoch 8/100, Loss: 0.5685
Epoch 9/100, Loss: 0.5662
Epoch 10/100, Loss: 0.5654
Epoch 11/100, Loss: 0.5590
Epoch 12/100, Loss: 0.5551
Epoch 13/100, Loss: 0.5550
Epoch 14/100, Loss: 0.5523
Epoch 15/100, Loss: 0.5481
Epoch 16/100, Loss: 0.5379
Epoch 17/100, Loss: 0.5210
Epoch 18/100, Loss: 0.5155
Epoch 19/100, Loss: 0.5122
Epoch 20/100, Loss: 0.5057
Epoch 21/100, Loss: 0.5136
Epoch 22/100, Loss: 0.5053
Epoch 23/100, Loss: 0.4993
Epoch 24/100, Loss: 0.4919
Epoch 25/100, Loss: 0.4931
Epoch 26/100, Loss: 0.5032
Epoch 27/100, Loss: 0.4865
Epoch 28/100, Loss: 0.4733
Epoch 29/100, Loss: 0.4679
Epoch 30/100, Loss: 0.4685
Epoch 31/100, Loss: 0.4298
Epoch 32/100, Loss: 0.4027
Epoch 33/100, Loss: 0.3682
Epoch 34/100, Loss: 0.3239
Epoch 35/100, Loss: 0.2829
Epoch 36/100, Loss: 0.2515
Epoch 37/100, Loss: 0.2219
Epoch 38/100, Loss: 0.1974
Epoch 39/100, Loss: 0.1736
Epoch 40/100, Loss: 0.1514
Epoch 41/100, Loss: 0.1379
Epoch 42/100, Loss: 0.1256
Epoch 43/100, Loss: 0.1124
Epoch 44/100, Loss: 0.1080
Epoch 45/100, Loss: 0.0994
Epoch 46/100, Loss: 0.0906
Epoch 47/100, Loss: 0.0877
Epoch 48/100, Loss: 0.0796
Epoch 49/100, Loss: 0.0751
Epoch 50/100, Loss: 0.0747
Epoch 51/100, Loss: 0.0678
Epoch 52/100, Loss: 0.0641
Epoch 53/100, Loss: 0.0630
Epoch 54/100, Loss: 0.0594
Epoch 55/100, Loss: 0.0580
Epoch 56/100, Loss: 0.0553
Epoch 57/100, Loss: 0.0573
Epoch 58/100, Loss: 0.0545
Epoch 59/100, Loss: 0.0497
Epoch 60/100, Loss: 0.0479
Epoch 61/100, Loss: 0.0488
Epoch 62/100, Loss: 0.0467
Epoch 63/100, Loss: 0.0459
Epoch 64/100, Loss: 0.0438
Epoch 65/100, Loss: 0.0440
Epoch 66/100, Loss: 0.0434
Epoch 67/100, Loss: 0.0388
Epoch 68/100, Loss: 0.0391
Epoch 69/100, Loss: 0.0379
Epoch 70/100, Loss: 0.0389
Epoch 71/100, Loss: 0.0372
Epoch 72/100, Loss: 0.0387
Epoch 73/100, Loss: 0.0369
Epoch 74/100, Loss: 0.0348
Epoch 75/100, Loss: 0.0335
Epoch 76/100, Loss: 0.0327
Epoch 77/100, Loss: 0.0340
Epoch 78/100, Loss: 0.0346
Epoch 79/100, Loss: 0.0322
Epoch 80/100, Loss: 0.0303
Epoch 81/100, Loss: 0.0321
Epoch 82/100, Loss: 0.0311
Epoch 83/100, Loss: 0.0293
Epoch 84/100, Loss: 0.0297
Epoch 85/100, Loss: 0.0301
Epoch 86/100, Loss: 0.0280
Epoch 87/100, Loss: 0.0291
Epoch 88/100, Loss: 0.0278
Epoch 89/100, Loss: 0.0256
Epoch 90/100, Loss: 0.0250
Epoch 91/100, Loss: 0.0261
Epoch 92/100, Loss: 0.0259
Epoch 93/100, Loss: 0.0240
Traceback (most recent call last):
  File "C:\Users\ediso\MISL\structure_to_signal\model.py", line 271, in <module>
    # criterion = nn.MSELoss()
                   ^^^^^^^^^^^
  File "C:\Users\ediso\MISL\structure_to_signal\model.py", line 106, in train
    epoch_loss += loss.item()
                  ^^^^^^^^^^^
KeyboardInterrupt